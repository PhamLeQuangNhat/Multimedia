WHY WE CAN, AND CAN NOT ZERO IN SOMETHING?

"Humans can only leverage a trivial proportion of brain capacity" has been so familiar a myth that has turned itself into a catchphrase. Its ubiquity has even urged such a fantasy concept as humans about with "superintelligence"; take, for example, "Limitless", and the more recent blockbuster "Lucy", buying into the % figure (of presupposed human brain's capacity) proposed by Professor Norman (casted by Morgan Freeman).

In all likelihood, it would appear vehemently absurd, were the most sophisticated, itself-having-its-size-tripled-after--million-years-of-evolution, and eating-up-%-of-the-energy-inputs biological structure of life to purely function at , or even % of its capacity. Such a figure would only come close to reality, were we to compare the number of neurons and neuroglia - cross-linked cells that make up the protective network) axons and neurotransmitters - with the latter fiercely overwhelming the former.

It would, were we to solely regard attention; or the very ability to process purely a minor selective amount of stimuli and information, thus discarding the major rest in the brain's environment.

How does attention work?

Given an inordinate number of treatises , which came hand in hand with as-convoluted explanations (from both cognitive- and neuroscience perspectives), the majority of which did merely zero in attention spotlights, thus heedless of the overall gloomy scenario. They skimmed through the background environment, homed in the very factors behind our actions and turned a blind eye to others (trivial environmental ones). The scrutiny, after all, serves as the very metaphor of visual spatial attention , and the so-called zoom-lens model.

Nevertheless, since we own our five senses, inasmuch as perceptual stimuli are hardly constrained to 'bare eyes' or 'receptive ears'. It is when E. C. Cherry's theory of selective auditory attention and cocktail party effect comes in handy, in which participants still converse and heedful of others calling their names, given the rambunctious background. Also, as you read these lines, your surroundings are perpetually breeding a number of perceptual stimuli: eye-catching, bizarre things penetrating your visual ranges, the strident sounds of cars, the aroma and sweet taste of freshly brewed tea, the murmur and chilling feelings stirred up by air conditioners; let alone our current flows of thoughts and buried-deep emotions.

All of which would only stand out once we stay heedful of which. Otherwise, they would die away into the background. Given that we might, to some extent, stay cognizant of everything, yet leave them deeply buried. To put into perspective, those participating in the cocktail party experiment themselves evidenced that we still [somewhat] picked up others' mention, albeit in a boisterous background . We, as a rule, do perceive such sounds, yet too dimly to become perceptible.

Myriad stimuli creep upon us as we glimpse the screen. At the very core of the brain, every letter, every piece of information, one after another, still is being chewed in our perception, whilst the aforementioned disturbers fade away, into the gray area of our feeble minds. Everything is being processed somewhat effortlessly, thriving on the very mechanism which discards information, in accordance with their relevance to the current tasks, as afore-argued in our Multitasking article . Until a more appealing stimulus emerges, thus changing the "spotlight" direction - take, for example, a cellphone message alert.

Theories of selective attention had it that the process of input acquisition could be, in general, depicted as a horizontal funnel. Such a metaphor simultaneously presented the two critical attention characteristics: limitive and selective. Given the violent external stimuli beleaguer, most of which, upon perceived, would become hard-stuck at a bottleneck, forasmuch as only a selected few - those of critical roles to current cognitive tasks - could 'pass'.

To comply with the attention restrictions, selection becomes a must, after all, to prevent the overloading information-processing capacity. To all appearances, given this "selective acquisition", the 'world' we experience might be solely a trivial part of the real world, for our own brains are turning a blind eye to an enormous amount of input data. We must have been all too familiar with the video experiment, wherein a hairy monkey was leisurely walking past the screen without getting caught, for the viewers were busy counting the white team's serves.

There are still differences between selective theories, which lied in how they interpreted the hard-stuck-at-the-bottleneck information. To demonstrate, Broadent's stated that they must have been wiped out, whilst Treisman suggested that those might have stayed, yet dwindled. Nevertheless, all of which have been fiercely criticized for their limitations, which, as a rule, paved the very way for the "spotlight" concept to later overwhelm any other argument/approach.

Are we actually blind, or failing to notice the noticeable?

Over the past few decades, many an attention study have zeroed in the cortex part - the higher-order cognition area of the brain. A recent study , however, indicated that the focal point must have been the very way brain repressed information, since the process did ultimately correlate the primitive brain regions woefully neglected by former studies.

Researchers proposed that the thalamic reticular nucleus (TRN), a thin layer of inhibitory nerve cells surrounding the thalamus - which originally received perceptual inputs in 'wake mode' and halted them otherwise) - somehow distorting the way animals channeled their attention onto certain objects, which was coevally tied up by the five senses.

Researchers, accordingly, trained mice to move in alignment with flashlight and sounds. Upon getting them exposed to both signals, yet giving conflicting commands (beforehand beckoning which to ignore, and leveraging techniques to deactivate the subjects'certain brain areas), they yielded quite a surprising result.

In an experiment where mice needed to home in visual information, activating visual TRN did downgrade their performances, given that deactivating which spearheaded difficulties zeroing in auditory information. In other simplified words, the network must have shouldered the responsibility of inhibiting non-critical information input. Were auditory information craved, visual TRN cells would stretch themselves out to inhibit the hypothalamus, thus doing away with redundant visual information. The findings, after all, suggest a better depiction of our attention system: a filter, instead of stage's lights.

Distraction is a "premium feature", instead of a bug.

While the number of research on the mechanisms of allocating task-focused attention still is woefully trivial, other studies have evidenced that our attention span has been 'quelled' since the dawn of the Internet, handheld devices and social networks (on which we as well have many a time argued).

Statistical data suggested that our average attention span had downgraded to seconds by, given the former seconds by . It, thus, took our mind less than seconds before it got dull and wandered for new interests.

% of the surveyed Americans did either fail to remember filling in basic information forms, or lose sight of an everyday item during the previous week. Officials did check emails times in-between a working hour. People, as a rule, turned to their phones times a week; an equivalence of hours minutes on a daily basis. Our concentration span, after all, is being spoiled by information redundancy, insomuch as disturbers are always creeping upon.

Another approach sees our very inherent zeroing-in capacity as tilted towards distracting factors.

Another study on humans and macaque monkeys did conclude that our attention, at its very core, had never been a continuous straight line, rather a perpetuated alternation between the two states of attention and distraction. The cycle could be illustrated as a graph with continuous sinusoidal curves, forming consecutive peaks (concentration) and troughs (distractions) with a frequency of roughly per second.

To put into perspective, however voraciously you read this article, there ARE moments, wherein you would pause yourselves to skim your surroundings for what-might-be-appear-more-critical before going back to these bone-dry arguments. The process, accordingly, repeats every seconds on average. To put it simply, our minds are doomed absent-minded for half the time we-think-we-are-conscious. Our real-world awareness, thus, is ending up perpetually interrupted. That either the interval has been too trivial, or our own brains are exceptional at self-deception, after all, has bred the very assumption of our perception as a harmonious process.

Digging further into the study's results, Ian Fiebelkorn, a scholar at Princeton University's School of Neuroscience (PNI), asserted that it was, in all likelihood, an evolutionary advantage upon interacting with the environment. Let us not leave behind the 'heydays' our foraging ancestors lived by terrifying prehistoric predators. Had they zeroed in crafting stone scabbards or arrowheads without staying heedful of the surrounding bushes, the -legged hunters must have unimpeachably ended up easy meals for their -legged colleagues.

With the pattern of continuously variable rhythms between the two similar states obtained on macaque monkey subjects, our neglect can now boast proudly that it is a factor that ensures survival. of mankind during its harsh evolution. Even when studying two tribes in Kenya in , Northwestern University researchers found that allele DRD R in DNA - related to attention deficit hyperactivity disorder (ADHD) has the frequency of occurrence in the tribes is more based on the nomadic hunting lifestyle compared to the communities living permanently in agriculture. Moreover, in the nomadic community, people carrying this allele R are less likely to experience low birth weight than those who do not. In contrast, in all the settlements, allenic carriers associated with ADHD were significantly smaller in weight than those who did not.

On the whole, without distraction, concentration must have appeared somewhat ill-favored and not-worth-pursuing - in a like manner to the clashing existences of Batman and Joker.

Still, do not get procacious of your being-distraction. For the modern society has got chalk and cheese with the primitive pastures, framing many a 'comfort zone' for concentration. Such a society adheres to, and always hurts for concentration, and those exceptional at which.

Given that we, after all, are 'designed' tilted towards disturbance.

To all appearances, this somehow reasons out why we are doomed to suffer, as our primitive instincts are perpetually going violently against social demands.  

